/*
 * ATTENTION: The "eval" devtool has been used (maybe by default in mode: "development").
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
/******/ (() => { // webpackBootstrap
/******/ 	var __webpack_modules__ = ({

/***/ "./provider/Game/game.js":
/*!*******************************!*\
  !*** ./provider/Game/game.js ***!
  \*******************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

"use strict";
eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Game: () => (/* binding */ Game)\n/* harmony export */ });\n\nclass Game{\n\n  constructor(gameOptions, oculusOptions, videoOptions){\n      console.log('%c[Game]', 'background: #7b1fa2; color: #fff', {THREE, gameOptions, oculusOptions, videoOptions});\n      this.videoOptions = videoOptions;\n      this.oculusOptions = oculusOptions;\n      this.gameOptions = gameOptions;\n      this.canvasR = gameOptions.canvasR;\n      this.canvasL = gameOptions.canvasL;\n\n      // Verificar se o WebGPU está disponível\n      if (!navigator.gpu) {\n          console.error('WebGPU is not supported in this browser.');\n          return;\n      }\n\n  }\n\n\n  settings(){\n          // Configurar a cena\n          const scene = new THREE.Scene();\n\n\n          const fov = 10;\n          const aspect = 2; // the canvas default\n          const near = 0.1;\n          const far = 100;\n    \n          // Configurar a câmera\n          const camera = new THREE.PerspectiveCamera( fov, aspect, near, far );\n          // const camera = new THREE.PerspectiveCamera(45, window.innerWidth / window.innerHeight, 0.1, 1000);\n          camera.position.set( 0, 0, 20 );\n          // camera.lookAt( 0, 0, 0 );\n  \n          // Configurar o renderizador\n       \n          const rendererL = new THREE.WebGLRenderer({antialias: false, alpha: true});\n          const rendererR = new THREE.WebGLRenderer({antialias: false, alpha: false});\n\n          // const controls = new OrbitControls( camera, rendererL.domElement );\n          \n  \n  \n          console.log('%c[Game]', 'background: #7b1fa2; color: #fff',{rendererL, innerWidth : window.innerWidth, innerHeight : window.innerHeight});\n          // renderer.setSize(window.innerWidth, window.innerHeight);\n  \n          rendererL.setPixelRatio(window.devicePixelRatio);\n          rendererR.setPixelRatio(window.devicePixelRatio);\n          // rendererL.setSize(window.innerWidth / 2, window.innerHeight / 2);\n          rendererL.setSize(this.videoOptions.video.element.videoWidth, this.videoOptions.video.element.videoHeight);\n          rendererR.setSize(this.videoOptions.video.element.videoWidth, this.videoOptions.video.element.videoHeight);\n  \n          // rendererL.domElement.style.position = 'absolute';\n          // console.log(this.videoOptions.video.element.videoWidth);\n          // rendererL.domElement.style.left = '38%';\n          // rendererL.domElement.style.top = this.canvasL.style.top;\n          rendererL.domElement.classList.add('renderer');\n  \n          rendererR.domElement.style.position = 'absolute';\n          // rendererR.domElement.style.right = this.oculusOptions.eyeRight.offsetX;\n          // rendererR.domElement.style.top = this.canvasL.style.top;\n          rendererR.domElement.classList.add('renderer');\n          rendererR.domElement.classList.add('full');\n  \n          // const contextCanvasR = document.querySelector('#overlay2').getContext('2d',  { willReadFrequently: true });\n          // contextCanvasR.drawImage(rendererL.domElement, 0, 0, this.canvasR.width, this.canvasR.height);\n  \n          console.log('%c[Game]', 'background: #7b1fa2; color: #fff', {width: this.canvasL.width});\n      \n          document.querySelector('.vr-container').appendChild(rendererL.domElement);\n          document.querySelector('.vr-container').appendChild(rendererR.domElement);\n\n          return {scene, camera, rendererL, rendererR};\n  \n  }\n\n  init(){\n\n\n      const {scene, camera, rendererL, rendererR} = this.settings();\n     \n      // Adicionar um cubo\n      // const geometry = new THREE.BoxGeometry(1, 1, 1, 64, 64, 64); // (1, 1, 1, 32, 32, 32) or (1, 1, 1, 8, 8, 8)\n      const geometry = new THREE.BoxBufferGeometry(1, 1, 1, 64, 64, 64); \n      const material = new THREE.MeshBasicMaterial({ color: 0x00ff00, wireframe: false });\n      const cube = new THREE.Mesh(geometry, material);\n      // scene.add(cube);\n\n\n      const materialLine = new THREE.LineBasicMaterial( { color: 0x0000ff } );\n      const points = [];\n      points.push( new THREE.Vector3( -1, 0, 0 ) );\n      points.push( new THREE.Vector3( 0, 1, 0 ) );\n      points.push( new THREE.Vector3( 1, 0, 0 ) );\n      const geometryLine = new THREE.BufferGeometry().setFromPoints( points );\n      const line = new THREE.Line( geometryLine, materialLine );\n      // scene.add( line );\n\n\n      // Adicionar uma luz\n      const light = new THREE.PointLight(0xffffff, 1, 100);\n      light.position.set(10, 10, 10);\n      scene.add(light);\n\n\n      // Carregar fonte e criar geometria de texto\n      var textMesh = null;\n      var loader = new THREE.FontLoader();\n      loader.load('https://threejs.org/examples/fonts/helvetiker_regular.typeface.json', function (font) {\n          var textGeometry = new THREE.TextGeometry('Hello, Three.js!', {\n              font: font,\n              size: 1,\n              height: 0.2,\n              curveSegments: 12,\n              bevelEnabled: true,\n              bevelThickness: 0.03,\n              bevelSize: 0.02,\n              bevelOffset: 0,\n              bevelSegments: 5\n          });\n\n          var textMaterial = new THREE.MeshBasicMaterial({ color: 0x00ff00 });\n          textMesh = new THREE.Mesh(textGeometry, textMaterial);\n          // scene.add(textMesh);\n      });\n\n\n      var loaderGL = new THREE.GLTFLoader();\n      loaderGL.load('../../es/source/Curious skeleton.glb', function (gltf) {\n          var model = gltf.scene;\n          scene.add(model);\n\n          // Opcional: animação do modelo\n          var mixer = new THREE.AnimationMixer(model);\n          gltf.animations.forEach((clip) => {\n              mixer.clipAction(clip).play();\n          });\n\n        \n          // Atualizar a animação\n          function animate_() {\n              requestAnimationFrame(animate_);\n\n              var delta = clock.getDelta();\n              mixer.update(delta);\n\n              rendererL.render(scene, camera);\n          }\n\n          var clock = new THREE.Clock();\n          animate_();\n      \n      }, undefined, function (error) {\n          console.error('Erro ao carregar o modelo:', error);\n      });\n\n\n\n\n      // Configurar Stats.js\n      const stats = new Stats();\n      stats.showPanel(0); // 0: fps, 1: ms, 2: mb\n      document.body.appendChild(stats.dom);\n\n      // Função de animação\n      function animate() {\n\n          stats.begin();\n\n          requestAnimationFrame(animate);\n\n          // Girar o cubo\n          cube.rotation.x += 0.01;\n          cube.rotation.y += 0.01;\n\n          // Atualizações para a animação\n          line.rotation.x += 0.01;\n          line.rotation.y += 0.01;\n\n          // Atualizações para a animação (se necessário)\n          if(textMesh){\n              textMesh.rotation.x += 0.01;\n              textMesh.rotation.y += 0.01;\n          }\n  \n\n          rendererL.render(scene, camera);\n          rendererR.render(scene, camera);\n\n          stats.end();\n      }\n\n      animate();\n\n      // Ajustar o tamanho da tela ao redimensionar a janela\n      window.addEventListener('resize', () => {\n          rendererL.setSize(window.innerWidth, window.innerHeight);\n          rendererR.setSize(window.innerWidth, window.innerHeight);\n\n          camera.aspect = window.innerWidth / window.innerHeight;\n          camera.updateProjectionMatrix();\n      });\n  \n\n  }\n}\n\n\n\n\n\n//# sourceURL=webpack:///./provider/Game/game.js?");

/***/ }),

/***/ "./provider/Oculus/Oculus.js":
/*!***********************************!*\
  !*** ./provider/Oculus/Oculus.js ***!
  \***********************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

"use strict";
eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Oculus: () => (/* binding */ Oculus)\n/* harmony export */ });\n\r\n\r\nif ('serviceWorker' in navigator) {\r\n    window.addEventListener('load', function() {\r\n    //   navigator.serviceWorker.register('/oculus-service-worker.js').then(function(registration) {\r\n    //     console.log('Service Worker registered with scope:', registration.scope);\r\n    //   }, function(error) {\r\n    //     console.log('Service Worker registration failed:', error);\r\n    //   });\r\n    });\r\n  }\r\n\r\n\r\n\r\n  class Oculus{\r\n    \r\n    constructor(oculusOptions, options = {}){\r\n\r\n        console.log('%c[Oculus]', 'background: blue; color: #fff',{oculusOptions, options});\r\n\r\n        this.iteration = 0;\r\n\r\n        this.defaultOptions  = {\r\n            api: 'https://cdn.jsdelivr.net/npm/@vladmandic/face-api@latest/dist/face-api.js',\r\n            urlRedirection: null,//'http://localhost:3000/aplication',\r\n            instanceof: null,\r\n            userInfo: { name: 'EDGARD', email: 'wqyZi@example.com' },\r\n            ratioConfimationSimilarity: 0.5,\r\n            settings: {\r\n                typeDetection: 'detectAllFaces',\r\n                choosefacedetecto:'SsdMobilenetv1Options', \r\n                inputSize:160, \r\n                scoreThreshold:0.5, \r\n                minConfidence:0.2,\r\n                methods : [\r\n                    { type: 'all', method: 'withFaceLandmarks',   draw: 'drawFaceLandmarks' }, // draw: 'drawFaceLandmarks'\r\n                    { type: 'all', method: 'withFaceExpressions', draw:  null }, // draw: 'drawFaceExpressions'\r\n                    { type: 'all', method: 'withAgeAndGender',    draw:  null },\r\n                    { type: 'detectSingleFace', method: 'withFaceDescriptor',  draw: null}, //draw: drawDetections\r\n                    { type: 'detectAllFaces',   method: 'withFaceDescriptors', draw: null,  } // draw: 'drawDetections'\r\n                ],\r\n                timeOutLoopAnalyse: 5000,\r\n            },\r\n            video: {\r\n                elementId: 'video' ,\r\n                size: 'ideal',\r\n                config: {\r\n                    width: { min: 640, ideal: 1280, max: 1920 },\r\n                    height: { min: 480, ideal: 720, max: 1080 },\r\n                    frameRate: { ideal: 30, max: 60 },\r\n                    aspectRatio: { ideal: 1.7777777778 }\r\n                },\r\n                // audio: {\r\n                //     echoCancellation: true,\r\n                //     noiseSuppression: true,\r\n                //     sampleRate: 44100\r\n                // },\r\n            },\r\n            canvas:{\r\n                elementId: 'overlay',\r\n                textFild: {\r\n                    text: \"Seu texto aqui\", \r\n                    x:50, y:50, \r\n                    fontSize:'30px', \r\n                    fontFamily:'Arial', \r\n                    color:'white'\r\n                },\r\n                config: {\r\n                    width: { min: 640, ideal: 1280, max: 1920 },\r\n                    height: { min: 361, ideal: 720, max: 1080 }\r\n                },\r\n            }\r\n         \r\n        }\r\n\r\n        // Merge custom options with default options\r\n        this.options = this.mergeOptions(this.defaultOptions, options);\r\n\r\n        this.eyeLeft = oculusOptions.eyeLeft.element;\r\n        this.eyeRight = oculusOptions.eyeRight.element;\r\n\r\n      \r\n        /*\r\n        this.eyeLeft.style.position = 'relative';\r\n        this.eyeRight.style.position = 'relative';\r\n        this.eyeLeft.style.left = `${oculusOptions.eyeLeft.offsetX}px`;\r\n        this.eyeLeft.style.top = `${oculusOptions.eyeLeft.offsetY}px`;\r\n        this.eyeRight.style.left = `${oculusOptions.eyeRight.offsetX}px`;\r\n        this.eyeRight.style.top = `${oculusOptions.eyeRight.offsetY}px`;\r\n\r\n        this.contextEyeRight = oculusOptions.eyeRight.element.getContext('2d',  { willReadFrequently: true });\r\n\r\n\r\n        oculusOptions.frameCanvasRight.element.style.position = 'absolute';\r\n        oculusOptions.frameCanvasRight.element.style.left = `${oculusOptions.eyeLeft.offsetX}px`;\r\n        oculusOptions.frameCanvasRight.element.style.top = `${oculusOptions.eyeLeft.offsetY}px`;\r\n        // oculusOptions.frameCanvasRight.element.style.zIndex = '2';\r\n\r\n        oculusOptions.frameCanvasLeft.element.style.position = 'absolute';\r\n        oculusOptions.frameCanvasLeft.element.style.right = `${oculusOptions.eyeRight.offsetX}px`;\r\n        oculusOptions.frameCanvasLeft.element.style.top = `${oculusOptions.eyeRight.offsetY}px`;\r\n        // oculusOptions.frameCanvasLeft.element.style.zIndex = '2';\r\n\r\n\r\n        this.options.video.elementRTC.style.position = 'absolute';\r\n        this.options.video.elementRTC.style.right = `${oculusOptions.eyeRight.offsetX}px`;\r\n        this.options.video.elementRTC.style.top = `${oculusOptions.eyeRight.offsetY}px`;\r\n\r\n        */\r\n\r\n        this.options = options;\r\n        this.frame = oculusOptions.frameCanvasRight.element\r\n        // this.context = oculusOptions.frameCanvasRight.element.getContext('2d',  { willReadFrequently: true });\r\n        // this.context2 = oculusOptions.frameCanvasLeft.element.getContext('2d' ,  { willReadFrequently: true });\r\n\r\n        this.context =  this.options.canvas.element.getContext('2d');\r\n        this.context2 = this.options.canvas.element2.getContext('2d');\r\n        this.context3 = this.options.canvas.element3.getContext('2d');\r\n        \r\n        this.handPose = null;\r\n        this.handsAnalyse = [];\r\n        this.oculusOptions = oculusOptions;\r\n\r\n        if(this.oculusOptions.model == \"NORMAL\"){\r\n\r\n        } \r\n\r\n\r\n  \r\n        \r\n    }\r\n\r\n    init(parans, callback){\r\n\r\n        this.createServerWorker();\r\n        this.setDetectHand();\r\n\r\n        if(typeof callback == 'function') callback(this);\r\n    }\r\n\r\n\r\n    createServerWorker() {\r\n     \r\n        this.worker = new Worker('handposeWorker.js');\r\n        console.log('%c[Oculus][Worker]', 'background: blue; color: #fff', {worker: this.worker});\r\n\r\n        this.worker.onmessage = (e) => {\r\n            this.handsAnalyse = e.data;\r\n            console.log('%c[Oculus][Worker]', 'background: purple; color: #fff', {handsAnalyse: this.handsAnalyse});\r\n            this.loopHandPoseDetector();\r\n        };\r\n    }\r\n\r\n    mergeOptions(defaultOptions, customOptions) {\r\n        // Recursive merge to handle nested objects\r\n        const mergedOptions = { ...defaultOptions };\r\n        for (const key in customOptions) {\r\n            if (customOptions[key] instanceof Object && key in defaultOptions) {\r\n                mergedOptions[key] = this.mergeOptions(defaultOptions[key], customOptions[key]);\r\n            } else {\r\n                mergedOptions[key] = customOptions[key];\r\n            }\r\n        }\r\n        return mergedOptions;\r\n    }\r\n\r\n    async createHandesDetector() {\r\n\r\n       \r\n        this.handPoseModel = handPoseDetection.SupportedModels.MediaPipeHands;\r\n        const detectorConfig = {\r\n            runtime: 'mediapipe', //'mediapipe', // or 'tfjs'\r\n            solutionPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/hands', //'https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh',\r\n            maxFaces: 10, // Número máximo de rostos a serem detectados\r\n            refineLandmarks: true, // Se true, melhora a precisão de alguns pontos-chave específicos (por exemplo, olhos)\r\n            modelType: 'full', // 'lite' ou 'full', onde 'lite' é mais rápido e 'full' é mais preciso\r\n        }\r\n   \r\n        return new Promise( async (resolve, reject) => {\r\n            this.handPoseDetector = await handPoseDetection.createDetector(this.handPoseModel, detectorConfig);\r\n            console.log('%c[Oculus][createHandesDetector][model]', 'background: blue; color: #fff', {model: this.handPoseModel, detectorConfig, detector: this.handPoseDetector});\r\n            resolve(this.handPoseDetector);\r\n        })\r\n        \r\n    }\r\n\r\n    async setDetectHand() {\r\n\r\n\r\n        const optionsModel = {\r\n            maxHands: 8,\r\n            flipped: false,\r\n            runtime: \"mediapipe\", //mediapipe, tfjs\r\n            modelType: \"lite\", //lite, full\r\n            detectorModelUrl: undefined, //default to use the tf.hub model\r\n            landmarkModelUrl: undefined, //default to use the tf.hub model\r\n        }\r\n\r\n        console.log('%c[Oculus][setDetectHand]', 'background: blue; color: #fff', {optionsModel});\r\n\r\n\r\n        if(typeof ml5 !== 'undefined'){\r\n            console.log('%c[Oculus][setDetectHand]', 'background: blue; color: #fff', \"model handPose loaded\", this.handPose);\r\n            console.log('%c[Oculus][setDetectHand]', 'background: blue; color: #fff', ml5.tf.engine().registryFactory);\r\n            ml5.tf.setBackend(\"webgpu\");\r\n            await ml5.tf.ready().then(() => {\r\n                console.log('%c[Oculus][setDetectHand]', 'background: blue; color: #fff',{ getBackend:  ml5.tf.getBackend() });\r\n            });\r\n        }\r\n\r\n        if(typeof tf !== 'undefined'){\r\n            console.log('%c[Oculus][setDetectHand][engine]', 'background: blue; color: #fff', {tf, version: tf.version, handPoseDetection});\r\n            console.log('%c[Oculus][setDetectHand][engine]', 'background: blue; color: #fff', tf.engine().registryFactory);\r\n            tf.setBackend(\"webgpu\");\r\n            await tf.ready().then(() => {\r\n                console.log('%c[Oculus][setDetectHand][getBackend]', 'background: blue; color: #fff',{ getBackend:  tf.getBackend() });\r\n            });\r\n        }\r\n\r\n        \r\n        \r\n        await this.createHandesDetector();\r\n       \r\n        const settings = async () => {\r\n            console.log('%c[Oculus][settings]', 'background: blue; color: #fff', {settings: this.settings});\r\n            this.flow()   \r\n        }\r\n\r\n        this.handPose = typeof ml5 !== 'undefined' && ml5 ? await ml5.handPose( optionsModel , settings) : this.flow() ;\r\n        \r\n        return this;\r\n    }\r\n\r\n    pushHandsAnalyse(handsAnalyse){\r\n        this.handsAnalyse = handsAnalyse;\r\n    }\r\n\r\n    async loopHandPoseDetector(){\r\n\r\n        if(!this.debugEndLoop){\r\n            console.log('%c[Oculus][loopHandPoseDetector]', 'background: blue; color: #fff', {handPose: this.handPose, detectHand: this.oculusOptions.detectHand, handPoseDetector: this.handPoseDetector});\r\n            this.debugEndLoop = true\r\n        }\r\n\r\n        if(this.handPoseDetector && \r\n        (!this.oculusOptions || !this.oculusOptions.detectHandWorker) && \r\n        (this.oculusOptions && this.oculusOptions.detectHand) ){\r\n            requestAnimationFrame( ()=> this.loopHandPoseDetector() );\r\n            this.handsAnalyse = await this.handPoseDetector.estimateHands(this.options.video.element);\r\n            // console.log(this.handsAnalyse);\r\n        }\r\n\r\n        if(this.oculusOptions && this.oculusOptions.detectHandWorker && this.oculusOptions.detectHand ){\r\n            const imageData = await this.context.getImageData(0, 0, this.frame.width, this.frame.height);\r\n            // console.log('%c[Oculus][flow][tensor]', 'background: yellow; color: #000', {imageData});\r\n            this.worker.postMessage({\r\n                imageData: imageData,\r\n            });\r\n        }\r\n\r\n        return this;\r\n    }\r\n\r\n    async flow(){\r\n\r\n      \r\n\r\n        if(this.handPose){\r\n            if(this.oculusOptions.detectHand){\r\n                this.handPose.detectStart(this.eyeLeft, (r) =>  this.pushHandsAnalyse(r))\r\n            }\r\n        }\r\n        \r\n    \r\n        this.loopHandPoseDetector();  \r\n        this.draw();\r\n    }\r\n\r\n    draw(){\r\n\r\n        if(this && typeof this.handsAnalyse !== 'undefined') {\r\n\r\n            if(!this.loopDebugEnd) {\r\n                console.log('%c[Oculus][draw]', 'background: blue; color: #fff', {handsAnalyse: this.handsAnalyse});\r\n                this.loopDebugEnd = true;\r\n            }\r\n\r\n            \r\n       \r\n            // Limpar o canvas\r\n            // this.context.clearRect(0, 0, this.frame.width, this.frame.height);\r\n            // this.context.drawImage(this.frame, 0, 0, this.frame.width, this.frame.height);\r\n\r\n            this.width = this.options.canvas.element.width;\r\n            this.height = this.options.canvas.element.height;\r\n            \r\n            //overlay-min\r\n            this.context.drawImage(this.options.video.element, 0, 0, this.width, this.height);\r\n            //overlay2\r\n            this.context2.drawImage(this.options.video.element, 0, 0, this.width, this.height);\r\n\r\n            // this.context3.drawImage(this.options.video.element, 0, 0, this.width, this.height);\r\n      \r\n\r\n            this.filter();\r\n\r\n\r\n\r\n            // Draw all the tracked hand points\r\n            for (let i = 0; i < this.handsAnalyse.length; i++) {\r\n                let hand = this.handsAnalyse[i];\r\n                for (let j = 0; j < hand.keypoints.length; j++) {\r\n                    let keypoint = hand.keypoints[j];\r\n\r\n                    this.context.fillStyle = 'rgb(0, 255, 0)';\r\n                    this.context.beginPath();\r\n                    this.context.arc(keypoint.x, keypoint.y, 10, 0, 2 * Math.PI);\r\n                    this.context.fill();\r\n\r\n                    this.context2.fillStyle = 'rgb(0, 255, 0)';\r\n                    this.context2.beginPath();\r\n                    this.context2.arc(keypoint.x, keypoint.y, 10, 0, 2 * Math.PI);\r\n                    this.context2.fill();\r\n                }\r\n            }\r\n\r\n            this.handsAnalyse.length = 0;\r\n\r\n        };\r\n\r\n        requestAnimationFrame( ()=> this.draw() );\r\n    \r\n    }\r\n\r\n    async filter(){\r\n        // Manipular os pixels para inverter as cores\r\n        const imageData = await this.context.getImageData(0, 0, this.width, this.height);\r\n\r\n    \r\n    \r\n        const data = imageData.data;\r\n\r\n        // filter invert\r\n        // for (let i = 0; i < data.length; i += 4) {\r\n        //     data[i] = 255 - data[i];     // Inverte o vermelho\r\n        //     data[i + 1] = 255 - data[i + 1]; // Inverte o verde\r\n        //     data[i + 2] = 255 - data[i + 2]; // Inverte o azul\r\n        // }\r\n\r\n        // filter grayscale\r\n        for (let i = 0; i < data.length; i += 4) {\r\n            const avg = (data[i] + data[i + 1] + data[i + 2]) / 3;\r\n            data[i] = avg;\r\n            data[i + 1] = avg;\r\n            data[i + 2] = avg;\r\n        }\r\n\r\n        // normalize\r\n        // for (let i = 0; i < data.length; i += 4) {\r\n        //     data[i] = data[i] / 255.0;\r\n        //     data[i + 1] = data[i + 1] / 255.0;\r\n        //     data[i + 2] = data[i + 2] / 255.0;\r\n        // }\r\n\r\n        function binarize(imageData, threshold) {\r\n            const data = imageData.data;\r\n        \r\n            for (let i = 0; i < data.length; i += 4) {\r\n                const avg = (data[i] + data[i + 1] + data[i + 2]) / 3;\r\n                const value = avg >= threshold ? 255 : 0;\r\n                data[i] = value;\r\n                data[i + 1] = value;\r\n                data[i + 2] = value;\r\n            }\r\n        \r\n            return imageData;\r\n        }\r\n\r\n        // binarize(imageData, 128);\r\n\r\n\r\n        function adjustLighting(imageData, brightness, contrast) {\r\n            const data = imageData.data;\r\n            const factor = (259 * (contrast + 255)) / (255 * (259 - contrast));\r\n        \r\n            for (let i = 0; i < data.length; i += 4) {\r\n                data[i] = factor * (data[i] - 128) + 128 + brightness;\r\n                data[i + 1] = factor * (data[i + 1] - 128) + 128 + brightness;\r\n                data[i + 2] = factor * (data[i + 2] - 128) + 128 + brightness;\r\n            }\r\n        \r\n            return imageData;\r\n        }\r\n\r\n        // adjustLighting(imageData, 0, 128);\r\n\r\n\r\n        function blurFilter(imageData) {\r\n            const width = imageData.width;\r\n            const height = imageData.height;\r\n            const data = imageData.data;\r\n            const outputData = new Uint8ClampedArray(data.length);\r\n            const kernel = [\r\n                [1 / 16, 1 / 8, 1 / 16],\r\n                [1 / 8, 1 / 4, 1 / 8],\r\n                [1 / 16, 1 / 8, 1 / 16]\r\n            ];\r\n        \r\n            function applyKernel(x, y) {\r\n                let r = 0, g = 0, b = 0;\r\n                for (let ky = -1; ky <= 1; ky++) {\r\n                    for (let kx = -1; kx <= 1; kx++) {\r\n                        const px = x + kx;\r\n                        const py = y + ky;\r\n                        if (px >= 0 && px < width && py >= 0 && py < height) {\r\n                            const offset = ((py * width) + px) * 4;\r\n                            r += data[offset] * kernel[ky + 1][kx + 1];\r\n                            g += data[offset + 1] * kernel[ky + 1][kx + 1];\r\n                            b += data[offset + 2] * kernel[ky + 1][kx + 1];\r\n                        }\r\n                    }\r\n                }\r\n                return [r, g, b];\r\n            }\r\n        \r\n            for (let y = 0; y < height; y++) {\r\n                for (let x = 0; x < width; x++) {\r\n                    const [r, g, b] = applyKernel(x, y);\r\n                    const offset = ((y * width) + x) * 4;\r\n                    outputData[offset] = r;\r\n                    outputData[offset + 1] = g;\r\n                    outputData[offset + 2] = b;\r\n                    outputData[offset + 3] = data[offset + 3]; // Alpha channel remains unchanged\r\n                }\r\n            }\r\n        \r\n            return outputData;\r\n        }\r\n\r\n        // let outputData  = blurFilter(imageData);\r\n        // console.log({outputData});\r\n\r\n\r\n\r\n        function sobelFilter(imageData) {\r\n            const width = imageData.width;\r\n            const height = imageData.height;\r\n            const sobelData = [];\r\n            const grayscaleData = [];\r\n        \r\n            // Convert to grayscale\r\n            for (let i = 0; i < data.length; i += 4) {\r\n                const avg = (data[i] + data[i + 1] + data[i + 2]) / 3;\r\n                grayscaleData.push(avg, avg, avg, 255);\r\n            }\r\n        \r\n            // Sobel convolution\r\n            const sobelX = [\r\n                [-1, 0, 1],\r\n                [-2, 0, 2],\r\n                [-1, 0, 1]\r\n            ];\r\n        \r\n            const sobelY = [\r\n                [-1, -2, -1],\r\n                [0, 0, 0],\r\n                [1, 2, 1]\r\n            ];\r\n        \r\n            for (let y = 0; y < height; y++) {\r\n                for (let x = 0; x < width; x++) {\r\n                    const pixelX = (\r\n                        (sobelX[0][0] * getPixel(grayscaleData, x - 1, y - 1, width)) +\r\n                        (sobelX[0][1] * getPixel(grayscaleData, x, y - 1, width)) +\r\n                        (sobelX[0][2] * getPixel(grayscaleData, x + 1, y - 1, width)) +\r\n                        (sobelX[1][0] * getPixel(grayscaleData, x - 1, y, width)) +\r\n                        (sobelX[1][1] * getPixel(grayscaleData, x, y, width)) +\r\n                        (sobelX[1][2] * getPixel(grayscaleData, x + 1, y, width)) +\r\n                        (sobelX[2][0] * getPixel(grayscaleData, x - 1, y + 1, width)) +\r\n                        (sobelX[2][1] * getPixel(grayscaleData, x, y + 1, width)) +\r\n                        (sobelX[2][2] * getPixel(grayscaleData, x + 1, y + 1, width))\r\n                    );\r\n        \r\n                    const pixelY = (\r\n                        (sobelY[0][0] * getPixel(grayscaleData, x - 1, y - 1, width)) +\r\n                        (sobelY[0][1] * getPixel(grayscaleData, x, y - 1, width)) +\r\n                        (sobelY[0][2] * getPixel(grayscaleData, x + 1, y - 1, width)) +\r\n                        (sobelY[1][0] * getPixel(grayscaleData, x - 1, y, width)) +\r\n                        (sobelY[1][1] * getPixel(grayscaleData, x, y, width)) +\r\n                        (sobelY[1][2] * getPixel(grayscaleData, x + 1, y, width)) +\r\n                        (sobelY[2][0] * getPixel(grayscaleData, x - 1, y + 1, width)) +\r\n                        (sobelY[2][1] * getPixel(grayscaleData, x, y + 1, width)) +\r\n                        (sobelY[2][2] * getPixel(grayscaleData, x + 1, y + 1, width))\r\n                    );\r\n        \r\n                    const magnitude = Math.sqrt((pixelX * pixelX) + (pixelY * pixelY)) >>> 0;\r\n        \r\n                    sobelData.push(magnitude, magnitude, magnitude, 255);\r\n                }\r\n            }\r\n        \r\n            return sobelData;\r\n        }\r\n        \r\n        function getPixel(data, x, y, width) {\r\n            if (x < 0 || x >= width || y < 0 || y >= width) {\r\n                return 0;\r\n            }\r\n            return data[(y * width + x) * 4];\r\n        }\r\n\r\n        // sobelFilter(imageData);\r\n\r\n\r\n\r\n        function isolateColor(imageData, targetColor, tolerance) {\r\n            const width = imageData.width;\r\n            const height = imageData.height;\r\n            const data = imageData.data;\r\n            const outputData = new Uint8ClampedArray(data.length);\r\n        \r\n            for (let i = 0; i < data.length; i += 4) {\r\n                const r = data[i];\r\n                const g = data[i + 1];\r\n                const b = data[i + 2];\r\n        \r\n                // Calcular a distância da cor alvo\r\n                const distance = Math.sqrt(\r\n                    (r - targetColor.r) * (r - targetColor.r) +\r\n                    (g - targetColor.g) * (g - targetColor.g) +\r\n                    (b - targetColor.b) * (b - targetColor.b)\r\n                );\r\n        \r\n                if (distance < tolerance) {\r\n                    // Manter a cor original\r\n                    outputData[i] = r;\r\n                    outputData[i + 1] = g;\r\n                    outputData[i + 2] = b;\r\n                    outputData[i + 3] = data[i + 3]; // Alpha channel remains unchanged\r\n                } else {\r\n                    // Definir como preto (ou qualquer outra cor de fundo)\r\n                    outputData[i] = 0;\r\n                    outputData[i + 1] = 0;\r\n                    outputData[i + 2] = 0;\r\n                    outputData[i + 3] = data[i + 3]; // Alpha channel remains unchanged\r\n                }\r\n            }\r\n        \r\n            return new ImageData(outputData, width, height);\r\n        }\r\n\r\n        // let imageDataColor = isolateColor(imageData, { r: 255, g: 0, b: 0 }, 200);\r\n\r\n        this.context3.putImageData(imageData, 0, 0);\r\n        // this.context2.putImageData(imageData, 0, 0);\r\n   \r\n    }\r\n\r\n}\r\n\r\n \r\n\r\n\n\n//# sourceURL=webpack:///./provider/Oculus/Oculus.js?");

/***/ }),

/***/ "./provider/Video/Video.js":
/*!*********************************!*\
  !*** ./provider/Video/Video.js ***!
  \*********************************/
/***/ (() => {

eval("\r\nclass Video{\r\n    options = {\r\n        api: 'https://cdn.jsdelivr.net/npm/@vladmandic/face-api@latest/dist/face-api.js',\r\n        urlRedirection: null,//'http://localhost:3000/aplication',\r\n        instanceof: null,\r\n        userInfo: { name: 'EDGARD', email: 'wqyZi@example.com' },\r\n        ratioConfimationSimilarity: 0.5,\r\n        settings: {\r\n            typeDetection: 'detectAllFaces',\r\n            choosefacedetecto:'SsdMobilenetv1Options', \r\n            inputSize:160, \r\n            scoreThreshold:0.5, \r\n            minConfidence:0.2,\r\n            methods : [\r\n                { type: 'all', method: 'withFaceLandmarks',   draw: 'drawFaceLandmarks' }, // draw: 'drawFaceLandmarks'\r\n                { type: 'all', method: 'withFaceExpressions', draw:  null }, // draw: 'drawFaceExpressions'\r\n                { type: 'all', method: 'withAgeAndGender',    draw:  null },\r\n                { type: 'detectSingleFace', method: 'withFaceDescriptor',  draw: null}, //draw: drawDetections\r\n                { type: 'detectAllFaces',   method: 'withFaceDescriptors', draw: null,  } // draw: 'drawDetections'\r\n            ],\r\n            timeOutLoopAnalyse: 5000,\r\n        },\r\n        video: {\r\n            elementId: 'video' ,\r\n            size: 'ideal',\r\n            config: {\r\n                width: { min: 640, ideal: 1280, max: 1920 },\r\n                height: { min: 480, ideal: 720, max: 1080 },\r\n                frameRate: { ideal: 30, max: 60 },\r\n                aspectRatio: { ideal: 1.7777777778 },\r\n                facingMode: 'environment'\r\n            },\r\n            // audio: {\r\n            //     echoCancellation: true,\r\n            //     noiseSuppression: true,\r\n            //     sampleRate: 44100\r\n            // },\r\n        },\r\n        canvas:{\r\n            elementId: 'overlay',\r\n            textFild: {\r\n                text: \"Seu texto aqui\", \r\n                x:50, y:50, \r\n                fontSize:'30px', \r\n                fontFamily:'Arial', \r\n                color:'white'\r\n            },\r\n            config: {\r\n                width: { min: 640, ideal: 1280, max: 1920 },\r\n                height: { min: 361, ideal: 720, max: 1080 }\r\n            },\r\n        } \r\n    }\r\n\r\n    constructor({api, instanceofApi, settings, video, canvas, userInfo, urlRedirection, ratioConfimationSimilarity} = this.options) {\r\n        this.userInfo = userInfo;\r\n        this.loopDetection = true;\r\n        this.quantityConfimationSimilarity = 100;\r\n        this.maxFacesToSave = 10;\r\n        this.minDetectionFaceScore = 0.80\r\n        this.minSimilarityThreshold = 0.20;\r\n        this.minAverageDistance = 0.20;\r\n        this.ratioConfimationSimilarity = ratioConfimationSimilarity;\r\n        this.urlRedirection = urlRedirection;\r\n        this.api = api;\r\n        this.faceapi = instanceofApi;\r\n        this.settings = settings;\r\n        this.methods = settings.methods;\r\n        this.video = video;\r\n        this.sizeVideo = video.size;\r\n        this.canvas = canvas;\r\n        this.faceMatcher = null;\r\n        this.mediaRecorder = null\r\n        this.faceDataset = [];\r\n        this.faceDatasetQualityHigh = [];\r\n        this.faceDescriptors = [];\r\n        this.imgDataset = [];\r\n        this.recordedChunks = [];\r\n        this.detectionToUploadJson = [];\r\n        this.detectionJsonToTest = [];\r\n        this.detectionsWebCam = [];\r\n        this.canvasDatasetTest = [];\r\n        this.elementsIMG = [];\r\n        this.arrAverage = [];\r\n        this.faceCanvasExtracted = [];\r\n        this.userDetectName = 'Face analyse';\r\n        this.boxBorderColor = 'blue';\r\n        this.confimationSimilarity = 0;\r\n        this.faceCount = 0;     \r\n        this.debug = false;\r\n        this.debugEnd = 0\r\n        this.container = null;\r\n    }\r\n\r\n\r\n    async checkCameraPermission() {\r\n        // Verifica se a câmera está disponível\r\n        if (!navigator.mediaDevices || !navigator.mediaDevices.enumerateDevices) {\r\n            alert(\"Seu navegador não suporta a detecção de dispositivos de mídia. Por favor, use um navegador mais moderno.\");\r\n            return;\r\n        }\r\n\r\n        // Verifica as permissões da câmera\r\n        navigator.mediaDevices.enumerateDevices()\r\n        .then(function(devices) {\r\n            let cameraExists = devices.some(device => device.kind === 'videoinput');\r\n\r\n            if (!cameraExists) {\r\n                alert(\"Nenhuma câmera foi encontrada. Por favor, conecte uma câmera.\");\r\n                return;\r\n            }\r\n\r\n            navigator.permissions.query({ name: 'camera' })\r\n                .then(function(permissionStatus) {\r\n                    if (permissionStatus.state === 'granted') {\r\n                        console.log(\"Permissão para usar a câmera concedida.\");\r\n                    } else if (permissionStatus.state === 'prompt' || permissionStatus.state === 'denied') {\r\n                        alert(\"Permissão para usar a câmera necessária. Por favor, permita o uso da câmera nas configurações do seu navegador.\\n\\nInstruções:\\n1. Clique no ícone de cadeado na barra de endereço do navegador.\\n2. Vá até 'Permissões' ou 'Configurações do site'.\\n3. Encontre 'Câmera' e selecione 'Permitir'.\");\r\n                    }\r\n                })\r\n                .catch(function(error) {\r\n                    console.error(\"Erro ao verificar permissões da câmera:\", error);\r\n                });\r\n        })\r\n        .catch(function(error) {\r\n            console.error(\"Erro ao enumerar dispositivos:\", error);\r\n            alert(\"Erro ao acessar dispositivos de mídia. Por favor, verifique as configurações do seu navegador e tente novamente.\");\r\n        });\r\n    }\r\n\r\n    async init() {\r\n        this.createVideoAndCanvas('#app', this.video, this.canvas)\r\n        .then(async () => {\r\n            await this.setupCamera({});\r\n            await this.canvasBackground({})\r\n           \r\n        }).catch(error => {\r\n            console.error('Erro 501',error);\r\n            alert(error);\r\n        });\r\n       \r\n        return this\r\n        \r\n    }\r\n\r\n    createVideoAndCanvas(containerSelector, videoRef, canvasRef) {\r\n        return new Promise((resolve, reject) => {\r\n            // Seleciona o contêiner onde os elementos serão adicionados\r\n            const container = document.querySelector(containerSelector);\r\n            this.container = container;\r\n            // container.style.width = canvasRef.config.width[this.sizeVideo] + 'px';\r\n            // container.style.height = canvasRef.config.height[this.sizeVideo] + 'px';\r\n            console.log({container});\r\n            \r\n            if (!container) {\r\n                console.error(`Container with selector \"${containerSelector}\" not found.`);\r\n                reject(`Container with selector \"${containerSelector}\" not found.`);\r\n                return;\r\n            }\r\n            \r\n            // Cria o elemento de vídeo\r\n            const video = document.createElement('video');\r\n            video.id = videoRef.elementId ||  'video';\r\n            video.autoplay = true;\r\n            video.muted = true;\r\n            video.width = videoRef.config.width[this.sizeVideo];\r\n            video.height = videoRef.config.height[this.sizeVideo];\r\n\r\n            console.log({video});\r\n\r\n            // Cria o elemento canvas\r\n            const canvas = document.createElement('canvas');\r\n            canvas.id = canvasRef.elementId ||  'overlay';\r\n            canvas.width = canvasRef.config.width[this.sizeVideo];\r\n            canvas.height = canvasRef.config.height[this.sizeVideo];\r\n\r\n            console.log({canvas});\r\n\r\n            // Cria o elemento canvas miniatura\r\n            const canvasMin = document.createElement('canvas');\r\n            canvasMin.id = canvas.id+'-min'\r\n            canvasMin.width = 80;\r\n            canvasMin.height = 80;\r\n\r\n            console.log({canvasMin});\r\n         \r\n            \r\n            // Adiciona os elementos ao contêiner\r\n            container.appendChild(video);\r\n            container.appendChild(canvas);\r\n            container.appendChild(canvasMin);\r\n      \r\n\r\n            this.video.element = video\r\n            this.canvas.element = canvas\r\n            this.canvas.canvasmin = canvasMin\r\n\r\n            this.checkCameraPermission();\r\n\r\n            resolve({ video, canvas });\r\n        });\r\n    }\r\n\r\n    async setupCamera({ video = this.video }, callback) {\r\n        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {\r\n            throw new Error('Browser API navigator.mediaDevices.getUserMedia not available');\r\n        }\r\n        \r\n        const stream = await navigator.mediaDevices.getUserMedia({ video: video.config, audio: video.audio });\r\n        console.log({stream});\r\n        video.element.srcObject = stream;\r\n        console.log({srcObject:video.element.srcObject});\r\n\r\n        \r\n        return new Promise((resolve, reject) => {\r\n            try {\r\n                video.element.onloadedmetadata = async () => {\r\n\r\n                    this.canvas.element.classList.add('remove-css-init'); \r\n                    \r\n                    console.log('%c[Camera on]', 'background: green; color: #fff', {\r\n                        video, \r\n                        videoHeight: video.element.videoHeight, \r\n                        videoWidth: video.element.videoWidth, \r\n                        width : video.element.width, \r\n                        height : video.element.height,\r\n                        size: this.sizeVideo,\r\n                    });\r\n\r\n                    if(typeof callback == 'function') callback({video, canvas: this.canvas});\r\n                    resolve(video.element)\r\n\r\n                    \r\n                    \r\n                };\r\n            } catch (error) {\r\n                console.error('Error 502 accessing camera:', error);\r\n                reject(error);\r\n            }\r\n        });\r\n    }\r\n\r\n    displaySize({ video = this.video, size = this.sizeVideo, element = 'video', canvas = this.canvas } = {}) {\r\n        if(element == 'video')\r\n            return { width: video.config.width[size], height: video.config.height[size] };\r\n        else \r\n            return { width: canvas.config.width[size], height: canvas.config.height[size] };\r\n    }\r\n\r\n    async canvasBackground({ canvas = this.canvas } = {}) {\r\n        canvas.element.width =  this.displaySize({}).width;\r\n        canvas.element.height = this.displaySize({}).height;\r\n        \r\n\r\n        // Obtém o contexto 2D do canvas e preenche o fundo com a cor vermelha\r\n        const context = canvas.element.getContext('2d');\r\n        context.fillStyle = 'transparent';\r\n        context.fillRect(0, 0, canvas.element.width, canvas.element.height);\r\n\r\n        // Define as propriedades do texto e adiciona o texto ao canvas\r\n        context.fillStyle = canvas.textFild.color;\r\n        context.font = `${canvas.textFild.fontSize} ${canvas.textFild.fontFamily}`;\r\n        context.fillText(canvas.textFild.text, canvas.textFild.x, canvas.textFild.y);\r\n\r\n        this.contextCanvasVideo = context;\r\n        return {canvas, context};\r\n    }\r\n}\n\n//# sourceURL=webpack:///./provider/Video/Video.js?");

/***/ }),

/***/ "./src/main.js":
/*!*********************!*\
  !*** ./src/main.js ***!
  \*********************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

"use strict";
eval("__webpack_require__.r(__webpack_exports__);\n/* harmony import */ var _provider_Video_Video_js__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../provider/Video/Video.js */ \"./provider/Video/Video.js\");\n/* harmony import */ var _provider_Video_Video_js__WEBPACK_IMPORTED_MODULE_0___default = /*#__PURE__*/__webpack_require__.n(_provider_Video_Video_js__WEBPACK_IMPORTED_MODULE_0__);\n/* harmony import */ var _provider_Oculus_Oculus_js__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../provider/Oculus/Oculus.js */ \"./provider/Oculus/Oculus.js\");\n/* harmony import */ var _provider_Game_game_js__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ../provider/Game/game.js */ \"./provider/Game/game.js\");\n\r\n\r\n\r\n\r\n\r\nconst videoConstructor = new _provider_Video_Video_js__WEBPACK_IMPORTED_MODULE_0__.Video();\r\nvideoConstructor.createVideoAndCanvas('.vr-container', videoConstructor.video, videoConstructor.canvas)\r\n.then(async (videoInstance) => {\r\n    console.log('%c[Video]', 'background: #222; color: #bada55', {videoInstance});\r\n\r\n\r\n    await videoInstance.setupCamera({}, async () => {\r\n        console.log('%c[Video] [setupCamera] [callback]', 'background: #222; color: #bada55', {videoInstance});\r\n        const { canvas, context } = await videoInstance.canvasBackground({})\r\n  \r\n\r\n  \r\n        \r\n        const eyeLeft = videoInstance.video.element;\r\n        const eyeRight = videoInstance.canvas.element;\r\n        eyeLeft.classList.add('eye_left');\r\n        eyeRight.classList.add('eye_right');\r\n\r\n       \r\n      \r\n        const oculusOptions = {\r\n            eyeLeft: {\r\n                element:eyeLeft,\r\n                offsetX: -60,\r\n                offsetY: -100,\r\n            }, \r\n            eyeRight:{\r\n                element:eyeRight,\r\n                offsetX: 0,\r\n                offsetY: -100,\r\n            },\r\n            frameCanvasRight:{\r\n              element: videoInstance.canvas.element,\r\n            },\r\n            frameCanvasLeft:{\r\n              element: videoInstance.canvas.element2\r\n            },\r\n            detectHand: true,\r\n            detectHandWorker: false,\r\n            model: \"NORMAL\", // NORMAL, GAME, VR\r\n        };\r\n\r\n        \r\n\r\n        console.log('%c[Oculus]', 'background: blue; color: #fff', {oculusOptions});\r\n   \r\n        const oculus = new _provider_Oculus_Oculus_js__WEBPACK_IMPORTED_MODULE_1__.Oculus(oculusOptions, videoInstance.options);\r\n        oculus.init(null, ()=>{\r\n\r\n            const gameOptions = {\r\n                canvasR: videoInstance.canvas.element2,\r\n                canvasL: videoInstance.canvas.canvasmin,\r\n            }\r\n            \r\n            const game = new _provider_Game_game_js__WEBPACK_IMPORTED_MODULE_2__.Game(gameOptions, oculusOptions, videoInstance.options);\r\n            game.init();\r\n        });\r\n\r\n\r\n\r\n\r\n        \r\n        function drawVideoToCanvas() {\r\n\r\n            // if(oculusOptions.model == \"NORMAL\" || oculusOptions.model == \"VR\"){\r\n            //     // canvas overlay\r\n            //     context.drawImage(videoInstance.video.element, 0, 0, videoInstance.canvas.element.width, videoInstance.canvas.element.height);\r\n            // }\r\n            \r\n            requestAnimationFrame(drawVideoToCanvas);\r\n        }\r\n\r\n        requestAnimationFrame(drawVideoToCanvas);\r\n\r\n\r\n    \r\n\r\n    });\r\n\r\n})\r\n.catch(error => {\r\n    console.error('Erro 501',error);\r\n    alert(error);\r\n});\n\n//# sourceURL=webpack:///./src/main.js?");

/***/ })

/******/ 	});
/************************************************************************/
/******/ 	// The module cache
/******/ 	var __webpack_module_cache__ = {};
/******/ 	
/******/ 	// The require function
/******/ 	function __webpack_require__(moduleId) {
/******/ 		// Check if module is in cache
/******/ 		var cachedModule = __webpack_module_cache__[moduleId];
/******/ 		if (cachedModule !== undefined) {
/******/ 			return cachedModule.exports;
/******/ 		}
/******/ 		// Create a new module (and put it into the cache)
/******/ 		var module = __webpack_module_cache__[moduleId] = {
/******/ 			// no module.id needed
/******/ 			// no module.loaded needed
/******/ 			exports: {}
/******/ 		};
/******/ 	
/******/ 		// Execute the module function
/******/ 		__webpack_modules__[moduleId](module, module.exports, __webpack_require__);
/******/ 	
/******/ 		// Return the exports of the module
/******/ 		return module.exports;
/******/ 	}
/******/ 	
/************************************************************************/
/******/ 	/* webpack/runtime/compat get default export */
/******/ 	(() => {
/******/ 		// getDefaultExport function for compatibility with non-harmony modules
/******/ 		__webpack_require__.n = (module) => {
/******/ 			var getter = module && module.__esModule ?
/******/ 				() => (module['default']) :
/******/ 				() => (module);
/******/ 			__webpack_require__.d(getter, { a: getter });
/******/ 			return getter;
/******/ 		};
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/define property getters */
/******/ 	(() => {
/******/ 		// define getter functions for harmony exports
/******/ 		__webpack_require__.d = (exports, definition) => {
/******/ 			for(var key in definition) {
/******/ 				if(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {
/******/ 					Object.defineProperty(exports, key, { enumerable: true, get: definition[key] });
/******/ 				}
/******/ 			}
/******/ 		};
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/hasOwnProperty shorthand */
/******/ 	(() => {
/******/ 		__webpack_require__.o = (obj, prop) => (Object.prototype.hasOwnProperty.call(obj, prop))
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/make namespace object */
/******/ 	(() => {
/******/ 		// define __esModule on exports
/******/ 		__webpack_require__.r = (exports) => {
/******/ 			if(typeof Symbol !== 'undefined' && Symbol.toStringTag) {
/******/ 				Object.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });
/******/ 			}
/******/ 			Object.defineProperty(exports, '__esModule', { value: true });
/******/ 		};
/******/ 	})();
/******/ 	
/************************************************************************/
/******/ 	
/******/ 	// startup
/******/ 	// Load entry module and return exports
/******/ 	// This entry module can't be inlined because the eval devtool is used.
/******/ 	var __webpack_exports__ = __webpack_require__("./src/main.js");
/******/ 	
/******/ })()
;